---
layout: post
title:  "The Annotated Pixel-CNN"
date:   2020-08-04 19:52:13 +0100
categories: jekyll update
---

## Introduction
> Generative image modeling is a central problem in unsupervised learning.

> However, because images are high dimensional and highly structured, estimating the distribution of natural images is extremely challenging.

> One of the most important obstacles in generative modeling is building complex and expressive models that are also tractable and scalable. 

> One effective approach to tractably model a joint distribution of the pixels in the image is to cast it as a product of conditional distributions.

> The factorization turns the joint modeling problem into a sequence problem, where one learns to predict the next pixel given all the previously generated pixels.

Whilst recurrent neural networks are an obvious choice:

> We observe that Convolutional Neural Networks (CNN) can also be used as sequence model with a fixed dependency range, by using Masked convolutions. The PixelCNN architecture is a fully convolutional network of fifteen layers that preserves the spatial resolution of its input throughout the layers and outputs a conditional distribution at each location.

$$p(x_{i,R}|x<i)p(x_{i,G}|x<i, x_{i,R})p(x_{i,B}|x<i, x_{i,R}, x_{i,G})$$

In this post we will focus on Pixel-CNNs. They are simpler to implement and once we have grasped the concepts, we can move on to Pixel-RNNs in a later post.


## Predicting sequences with convolutional networks
>[Recurrent models] have a potentially unbounded dependency range within their receptive field. This comes with a computational cost as each state needs to be computed sequentially. One simple workaround is to make the receptive field large, but not unbounded. We can use standard convolutional layers to capture a bounded receptive field and compute features for all pixel positions at once.

<figure style="text-align: center;">
<img src="{{site.baseurl}}/assets/PixelCNN/paper_fig2_pixelcnn.png"
alt="figure showing prediction from PixelCNN conv layer" style="display: initial;">
<figcaption>
From Figure 2 of the paper
</figcaption>
</figure>


Can you explain how a convolutional layer can predict sequences in parallel?

<div markdown="0" class="collapse-conv-seq">
<p class='textblock'>
Each pixel of the output (leaving out the very first pixel) can be interpreted as a prediction conditioned on the input pixels above and to the left (leaving out the very last pixel).
</p>
</div>

> Note that the advantage of parallelization of the PixelCNN over the PixelRNN is only available during training or during evaluating of test images. The image generation process is sequential for both kinds of networks, as each sampled pixel needs to be given as input back into the network.

Can you think of any other issues with parallel prediction?

<div markdown="0" class="collapse-parallel-pred">
<p class='textblock'>
When predicting pixels in parallel, for each pixel the model should only have access to "past" pixels i.e. those above and to the left. Otherwise it would be able to cheat by having access information about elements it is supposed to be predicting.
</p>
</div>

## Masking
# Motivation
> Masks are adopted in the convolutions to avoid seeing the future context

> The $h$ features for each input position at every layer in the network are split into three parts, each corresponding to one of the RGB channels. 

> When predicting the R channel for the current pixel $x_i$, only the generated pixels left and above of $x_i$ can be used as context.

>When predicting the G channel, the value of the R channel can also be used as context in addition to the previously generated pixels. 

>Likewise, for the B channel, the values of both the R and G channels can be used. 

Let us consider an example with 3 feature maps. Step through the animation and try to determine which pixels from input feature maps will contribute to the prediction in each case. For clarity a single arrow is used for the above-and-left pixels to indicate they will be used. 

<div>
{%- include PixelCNN/slider.html -%}
</div>

# Masks

>To restrict connections in the network to these dependencies, we apply a mask ...

>We use two types of masks that we indicate with mask A and mask B

>Mask A is applied only to the first convolutional layer in a PixelRNN and restricts the connections to those neighboring pixels and to those colors in the current pixels that have already been predicted.

>On the other hand, mask B is applied to all the subsequent input-to-state convolutional transitions and relaxes the restrictions of mask A by also allowing the connection from a color to itself.

Why do we need to mask the input at position $i,j,c$ to predict the output at $i,j,c$ for the first layer but not for subsequent layers?
<div class='collapse-first-subsequent-layers'>
<div markdown='1' class='textblock'>
We only need to hide the true data. As the diagram shows, the output of the first layer at $i,j,c$, $X1[i,j,c]$ only depends on the previous pixels and not on the pixel $I[i,j,c]$ of the input image $I$, which is the target for position $i,j,c$. Since $X1[i,j,c]$ is therefore only connected to the valid context for position $i,j,c$, it can be used when predicting $X2[i,j,c]$. 
</div>
</div>

# Implementation
>The masks can be easily implemented by zeroing out the corresponding weights

This is easiest to see if we note that 3x3 convs are used in the Masked Conv layers and that "same" padding is added in order to preserve dimensions. Thus as the kernel slides over the image, the centre of a kernel will be aligned with the pixel whose value is going to be predicted. Like the feature maps are split into 3 corresponding to each colour, the $k \times k \times f_\text{in} \times f_\text{out} $ kernel can be split into 9 of size $k \times k \times (f_\text{in}/3) \times (f_\text{out}/3) $ kernels that map every colour to every other colour.

Applying the masking rules, how will the mask for a $3 x 3$ kernel from each of these groups look like for mask A and mask B? Assume, as noted above, that the centre will be aligned with the "present" pixel.

<div class="collapse-mask-viz">
<p class="textblock">
The <strong style="color:#555">dark pixels are 0 </strong> 
and the <strong style="color:#f0f0f0; background: black">light pixels are 1</strong>. Note for clarity the centre pixel is shown slightly lighter or darker.
</p>
<div markdown="1">
![Mask of type A for 7x7 kernel]({{site.baseurl}}/assets/PixelCNN/maskA_7x7.png)

![Mask of type A for 7x7 kernel]({{site.baseurl}}/assets/PixelCNN/maskB_3x3.png)
</div>
</div>
If you group together just the kernel centres, which correspond to the present pixel position you get what looks like an upper triangular matrix. For mask A everything above the diagonal is 1 and for mask B everything above and including the diagonal is 1. Actually it will be a block triangular matrix with blocks of size $f_\text{in}/3 \times f_\text{out}$. 

Now try implementing this, keeping in mind these points:
- The mask values will be the same for all the previous pixels.
- The mask values for the present pixel will have an will be matrix of $f_\text{in}/3 \times f_\text{out}$ blocks one or zero blocks that have an upper-triangular form. 
- It can be assumed that the kernel size is odd and the image height will be padded by h_k // 2 and width by w_k // 2 to preserve dimensions so that the kernel centre will always correspond to the present pixel

<div class="collapse-get_mask">
<div markdown="1">
```python 
def get_mask(kernel_size, features_in, features_out, mode='B'):
    assert (features_in % 3) == 0, features_in
    assert (features_out % 3) == 0, features_out
    
    # Assume centre of kernel corresponds to present pixel
    # which will be the case if kernel dims are odd
    # and "same" padding is used 
    h, w = kernel_size
    i_cent = h // 2
    j_cent = w // 2
    mask = np.ones((h, w, features_in, features_out))
    
    # all zeros to the left in the centre row
    mask[i_cent, (j_cent + 1):] = 0.
    # all zeros below
    mask[(i_cent + 1):] = 0.
    
    # clr2clr[i, j] indicates whether colour_i
    # in the previous layer is connected to colour_j in present
    # where colours are R,G,B.
    # Entries above the diagonal are always 1 and below always 0
    # since there is no backward flow.
    # For mask B a colour feeds into itself in the next layer
    # so the diagonal entries are 1 but for mask A they are 0
    # meaning that the colour can't feed into itself if mask A is used 
    clr2clr = np.triu(np.ones((3, 3)), k=1 if mode=='A' else 0)
    
    rgb_mask = np.repeat(np.repeat(clr2clr, features_in//3, axis=0), 
                     features_out//3, axis=1)
    
    mask[i_cent, j_cent] = rgb_mask
    
    return mask.astype('float32')
```
</div>
</div>

Try plotting the masks from the kernels.
<div class="collapse-plot-masks">
<div markdown="1">
```python
## Parameters to plot the image for mask A shown. Modify to plot different masks
mode = 'A'
k = 7
ksize= (k, k)
f_in = 15
f_out = 24

## Code to plot
m = get_mask(ksize, f_in, f_out, mode)
masks = [[m[:, :, i * f_in//3, j * f_out//3] 
         for j in range(3)]
         for i in range(3)]
masks = [np.pad(i, ([(0, 1)] * 2), constant_values=1)
      for i in 2 * np.stack(masks).reshape([-1, 3, 3])
      ]
masks = np.reshape(masks, ksize + (4, 4))

fig, axes = plt.subplots(3, 3, figsize=(8, 8))
for i, clr_in in enumerate('RGB'):
    for j, clr_out in enumerate('RGB'):
        ax = axes[i, j]
        kernel = deepcopy(m[:, :, i * f_in//3, j * f_out//3]) * 255
        mid = np.floor_divide(kernel.shape, 2)
        mid_val = kernel[mid[0], mid[1]] 
        kernel[mid[0], mid[1]] = 55 if mid_val == 0 else 200
        ax.pcolor(kernel, edgecolor='k', vmin=0, vmax=255, cmap='gray')
        if j == 0:
            ax.set_ylabel('$%s_{in}$'%clr_in, color=clr_in.lower(), fontsize=16)
        if i == 0:
            ax.set_xlabel('$%s_{out}$'%clr_out, color=clr_out.lower(), fontsize=16)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.invert_yaxis()
        ax.xaxis.set_label_position('top')
    
    plt.suptitle(f'Mask {mode} for a {k} x {k} kernel', fontsize=24)
```
</div>
</div>

## Masked convolution
Let us now implement a masked conv layer. A simple approach is to implement conv layer where the kernel is multiplied with the mask before the convolution step. 

<div class="collapse-MaskedConv2D ">
<div markdown="1">
```python 
class MaskedConv2D(tf.keras.layers.Layer):
    def __init__(self, kernel_size, filters_in, filters, act=None, mode='B'):
        super(MaskedConv2D, self).__init__()
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        else:
            self.kernel_size = kernel_size
        self.filters = filters
        self.mode = mode
        if act is not None:
            self.act = tf.keras.layers.Activation(act)

        self.kernel = self.add_weight(name='kernel', shape=(*self.kernel_size, filters_in, self.filters))
        self.bias = self.add_weight(name='bias', shape=(self.filters,))
        mask_kwargs = dict(kernel_size=self.kernel_size, 
                             features_in=filters_in,
                             features_out=self.filters,
                             mode = self.mode)
        self.mask = get_mask(**mask_kwargs)
        
    def call(self, x):
        kernel = self.kernel * self.mask
        out = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')
        out = tf.nn.bias_add(out, self.bias)
        if hasattr(self, 'act'):
            return self.act(out)
        return out
```
</div>
<p class='textblock'>
Note: for simplicity we are passing in the number of input channels. An alternative approach would be to redefine the `build` function so that the mask as well as the weights are initialised when the module is first called so that the number of filters is inferred from the input rather than having to be provided in advance.
</p>
</div>


# PixelCNN

Once masking is understood, the architecture is actually rather simple and does not require many lines of code.

> The first layer is a 7 × 7 convolution that uses the mask of type A ... The PixelCNN [then] uses convolutions of size 3 × 3 with a mask of type B. The top feature map is then passed through a couple of layers consisting of a Rectified Linear Unit (ReLU) and a 1×1 convolution. **For the CIFAR-10 and ImageNet experiments, these layers have 1024 feature maps**; for the MNIST experiment, the layers have 32 feature maps. Residual and layer-to-output connections are used across the layers...
[Emphasis added]

This is summarised in the following table:

![Table with details of PixelCNN architecture]({{site.baseurl}}/assets/PixelCNN/paper_table1_arch.png)

The residual block is as shown below:

![Residual block consisting of 2h-conv1x1-h, h-conv3x3-h, h-conv1x1-2h with ReLU after each layer]({{site.baseurl}}/assets/PixelCNN/paper_fig5_block.png)

Let us a implement a module `PixelCNNResBlock` based on the diagram that makes use of the `MaskedConv2D` layer.

<div class="collapse-PixelCNNResBlock">
  <div markdown="1">
  ```python
  class PixelCNNResBlock(tf.keras.models.Model):
      def __init__(self, filters_in, filters):
          super(PixelCNNResBlock, self).__init__()
          self.conv_in = MaskedConv2D(1, filters_in, filters, act='relu')
          self.conv_mid= MaskedConv2D(3, filters, filters, act='relu')
          self.conv_out = MaskedConv2D(1, filters, 2 * filters, act='relu')
          
      def __call__(self, x):
          out = self.conv_in(x)
          out = self.conv_mid(out)
          out = self.conv_out(out)
          return x + out
  ```
  </div>
</div>

Now we can construct the model. Reshape the output so that it has shape `[batch_size, height, width, 3, 256]` corresponding to the 256 possible intensities for each colour. We will call this `PixelRNN` and pass in the residual block as an argument making it possible to reuse the module to also build the recurrent models in the paper later. 

<div class="collapse-PixelRNN">
<div markdown="1">
```python
class PixelRNN(tf.keras.models.Model):
    def __init__(self, hidden_dim, out_dim, n_layers, pixel_layer):
        super(PixelRNN, self).__init__()
        hidden_dim = hidden_dim * 3
        out_dim = out_dim * 3
        
        self.input_conv = MaskedConv2D(kernel_size=7,
                                       filters_in=3,
                                       filters=2 * hidden_dim,
                                       mode='A')
            
        self.pixel_layer = [pixel_layer(2 * hidden_dim, hidden_dim) for _ in range(n_layers)]
        
        self.output_conv1 = MaskedConv2D(kernel_size=1, 
                                         filters_in=2 * hidden_dim,
                                         filters=out_dim)
        self.output_conv2 = MaskedConv2D(kernel_size=1, 
                                         filters_in=out_dim,
                                         filters=out_dim)
        
        self.final_conv = MaskedConv2D(kernel_size=1, 
                                       filters_in=out_dim,
                                       filters=256 * 3)
    
    def __call__(self, x):
        y = self.input_conv(x) 
        for layer in self.pixel_layer:
            y = layer(y)
            
        y = self.output_conv1(tf.nn.relu(y))
        y = self.output_conv2(tf.nn.relu(y))
        y = self.final_conv(y)
        
        y = tf.reshape(y, tf.concat([tf.shape(y)[:-1], [3, 256]], 0))
        return y
```
</div>
</div>

# Data
We will use the CIFAR10 dataset and load it into memory using keras. Then we write a simple function `get_dataset` to create a dataset that does the following
- Normalises the images by 255
- Shuffles the images if training
- Returns a float32 and int32 version of the image where the int32 version is used as the label.
<div class="collapse-get_dataset">
<div markdown="1">
```python 
def get_dataset(imgs, batch_size=16, mode='train'):
  def _map_fn(x):
      labels = tf.cast(x, tf.int32)
      imgs = tf.cast(x, tf.float32)
      imgs = imgs / 255.
      return imgs, labels
  ds = tf.data.Dataset.from_tensor_slices(imgs)
  if mode == 'train':
      ds = ds.shuffle(1024)
  ds = ds.repeat(-1)
  ds = ds.map(_map_fn)
  return ds.batch(batch_size)
```
</div>
</div>

# Losses and metrics
> All our models are trained and evaluated on the log-likelihood loss function coming from a discrete distribution.
> For the multinomial loss function we use the raw pixel color values as categories.

In other words, the model is trained with a softmax cross-entropy loss over the 256 classes for each of red, green and blue. 

> For CIFAR-10 and ImageNet we report negative log-likelihoods in bits per dimension. The total discrete log-likelihood is normalized by the dimensionality of the images (e.g., 32×32×3 = 3072 for CIFAR-10). These numbers are interpretable as the number of bits that a compression scheme based on this model would need to compress every RGB color value (van den Oord & Schrauwen, 2014b; Theis et al., 2015); in practice there is also a small overhead due to arithmetic coding.

How can you derive the negative log-likelihoods (NLL) in bits per dimension from the softmax loss?

<div class="collapse-NLL-bits-per-dim">
<div class="textblock">
<p markdown="1">
The softmax loss for $N$ RGB images of size $H \times W$ is as follows:

  $${L_\text{softmax}(I_\text{true}, I_\text{pred})}=
  \\-\frac{1}{3\cdot NHW}\log_e\prod_{i=0}^{N-1}\prod_{x=0}^{H-1}\prod_{y=0}^{W-1}p(x_{i,R}|x_{<i})p(x_{i,G}|x_{<i}, x_{i,R})p(x_{i,B}|x_{<i}, x_{i,R}, x_{i,G})$$

where $x$ is the true intensity. 
</p>
<p markdown="1" style="margin:0">
This is the normalized NLL in nats averaged across the images. Here the $\log$ has base of $e$ whereas to get it in bits per dimension we need to use a base of 2. Since $\log_2(x) = \log_e(x) / \log_e(2)$, simply dividing the softmax loss by $\log_e(2)$ e.g. `tf.log(2)` or `np.log(2)` gives you the NLL in bits normalized by the dimensionality of the images and averaged across the images.
</p>
</div>
</div>

# Training 
> RMSProp gives best convergence performance and is used for all experiments. The learning rate schedules were manually set for every dataset to the highest values that allowed fast convergence.

As further details were not provided here about the learning rates, I looked OpenAI's codebase for PixelCNN++ and found that they used Adam with a learning rate of 0.001 multiplied by 0.999995 after each step so that is what we will use here. 

Now we can write a a `train_step` and `test_step` function that adapt the following functions in the TensorFlow tutorial:

# Inference
Inference as noted before must be done sequentially. How do think it is done?
<div class="collapse-inference-steps">
<div markdown="1" class="textblock">
- Start with an image consisting of single red pixel samplied uniformly at random from {0, ..., 255} and predict a distribution for green at this position.
- Sample from this distribution to get a single value for green.
- Update the image with the green pixel and make a prediction for blue.
- Update the image with the blue pixel and make a prediction for red at the next pixel
- And so on, until the entire image is generated.
</div>
</div>

Now implement a function `generate_images`, which given an model and a number of images to generate, repeatedly calls the model to produce an image. 

<div class="collapse-generate_images">
<div markdown="1">
```python 
  def generate_images(self, n_images=1, iteration=0):
      img = np.zeros((n_images, *self.img_dims), dtype='float32')
      img[np.arange(n_images), 0, 0, 0] = np.random.choice(256, size=n_images) / 255.
      step = 0
      for row in range(img.shape[1]):
          for colm in range(img.shape[2]):
              for clr in range(3):
                  step += 1
                  
                  if (clr + colm + row) > 0:
                      result = self.sess.run(self.out, feed_dict={self.inp: img})
                      result_rgb = np.stack([np.random.choice(256, p=result[i, row, colm, clr])
                                                      for i in range(n_images)])
                      img[:, row, colm, clr] = result_rgb / 255.
      
      if self.summary:
          self.writer.add_summary(sess.run(self.sum, feed_dict={self.inp: img}), iteration)
      
      return img
```
</div>
</div>

# Putting it all together
Finally we can put this all together. Write a `train_model` function that does the following:
- Loads the CIFAR10 images
- Builds the datasets
- Calls `train_step` and adds a train loss summary at regular intervals
- Calls `test_step` after a window of your choice
- Generates images at intervals of your choice (don't do this too frequently or for too many images as it will slow down the training)
- Prints any output you want
- Saves checkpoints as you want 

Basically a usual training function but I'm listing everything here so that the code hidden below is easier to follow and as prompts for you to implement it yourself if you prefer.

Then add

```python
if __name__ == '__main__':
  train_model()
```

and run 

```bash
python3 train.py
```

# Results
It took over 5 days to run ~450K steps on single GPU for the NLL to get close to the reported values. I stopped it after that as I needed the GPU for other stuff. The test loss went down to about 3.22 which is a bit higher than the reported value in the paper. I have didn't measure the NLL over the entire training set at a fixed checkpoint and the plot is of the per batch values after each training set. However it can be seen that this converges to a similar level.

![Table showing results of different models on CIFAR10]({{site.baseurl}}/assets/PixelCNN/paper_table5_results.png)

<figure style="text-align: center;">
<img src="{{site.baseurl}}/assets/PixelCNN/train_loss.png"
alt="Curve of training loss for around 450k iterations" style="display: initial;">
</figure>

<figure style="text-align: center;">
<img src="{{site.baseurl}}/assets/PixelCNN/test_loss.png"
alt="Curve of test loss for around 450k iterations" style="display: initial;">
</figure>

For comparison note that the better performing PixelCNN++ model is said to take around 5 days on 8 GPUs with a batchsize of 16 (so a 8x the number of images per step as used here) to converge it its best value. However image quality seems to improve much sooner. The figure below shows images generated at various points between around 150k and 450k iterations (in PixelCNN order).

<figure style="text-align: center;">
<img src="{{site.baseurl}}/assets/PixelCNN/gen_imgs.png"
alt="Images generated at various points between around 150k and 450k iterations" style="display: initial;">
</figure>


Nevertheless it should be noted that the images don't match the quality of those that are generated via models such as GANs so don't be be worried or disappointed if your images don’t seem to resemble anything in the real world. We trade-off model interpretability for quality. Here are examples of images from the models trained in the paper and you can see what they have are plausible patterns of colours but they lack meaningful content.

![Table showing results of different models on CIFAR10]({{site.baseurl}}/assets/PixelCNN/paper_fig7_cifar10.png)