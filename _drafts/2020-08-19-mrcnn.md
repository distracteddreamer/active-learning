---
layout: post
title:  "The Annotated Mask-RCNN"
date:   2020-08-19 15:52:13 +0100
categories: jekyll update
---
Active mode coming soon!

## Overview
> Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector ... that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2).

The MaskRCNN and other models extend this approach to predict additional properties of objects in an image such as segmentations in the case of Mask-RCNN. 

## Bounding boxes

Bounding boxes can be represented in a number ways. We will use the $xyxy$ format where the box is represented by its top-left and bottom-right corner co-ordinates $[x_1, y_1, x_2, y_2]$  and the $whxy$  format where we specify the width, height, and $x$ and $y$ coordinates of the centre, $[w, h, x_\text{centre}, y_\text{centre}]$. Note that $x_2$ and $y_2$ are *inside* the box.

You can find a detailed guide to bounding boxes  here. 

# Instance

Whilst training the model we need pass around various properties of RoIs, for example the class_ids, scores, anchor boxes, predicted boxes, target boxes, etc. We need to keep them together in the same order. If we select a subset of any one of these we need select corresponding subsets of the rest. To faciliate these steps we define a data structure `Instance`. 

```python
class Instance(object):

    def __init__(self, fields):
        self.fields = fields

    def __getattr__(self, attr):
        # Allow keys to be accessed as attributes
        if attr in self.fields:
            return self.fields[attr]
        return super().__getattribute__(self, attr)

    def map(self, fn, *args, **kwargs):
        fields, arrs = zip(*self.fields.items())

        def _fn(x):
            x = Instance(dict(zip(fields, x)))
            x_new = fn(x, *args, **kwargs)
            return tuple(x_new.fields[field] for field in fields)

        arrs_new = tf.map_fn(elems=arrs, fn=_fn)
        return Instance(dict(zip(fields, arrs_new)))

    def masked_select(self, keep):
        _fields = {}
        for field, arr in self.fields:
            _fields[field] = tf.boolean_mask(arr, keep)
        return Instance(_fields)

    def pad(self, pad_size):
        _fields = {}

        for field, arr in self.fields:
            pad = tf.zeros([tf.rank(arr) - 1, 2], dtype=tf.int32)
            padding = tf.concat([
                tf.stack([[0, pad_size]]),
                pad], axis=0)
            _fields[field] = tf.pad(arr, padding)
        return Instance(_fields)

    def select(self, inds):
        _fields = {}
        for field, arr in self.fields:
            _fields[field] = tf.gather(arr, inds)
        return Instance(_fields)

    def select_fields(self, fields):
        return Instance({field: self.fields[field] for field in fields})
```

## Region Proposal Networks

> A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score. We model this process with a fully convolutional network, which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network, we assume that both nets share a common set of convolutional layers. 


Both the RPN and the Fast R-CNN detection network share a "backbone" feature extractors whose feature maps are used for the subsequent steps. In the original Faster R-CNN paper the extract feature maps from just the final convolutional layer of the backbone network but later works use feature pyramids. We will subsequently look at how feature pyramids are extracted.

> To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an n × n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature. This feature is fed into two sibling fully- connected layers—a box-regression layer (reg) and a box-classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large. This mini-network is illustrated at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an $n \times n$ convolutional layer followed by two sibling $1 \times 1$ convolutional layers (for reg and cls, respectively).

> At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as $k$. So the reg layer has 4k outputs encoding the coordinates of $k$ boxes, and the cls layer outputs $2k$ scores that estimate probability of object or not object for each proposal4. The $k$ proposals are parameterized relative to $k$ reference boxes, which we call anchors. 

> In our formulation, the features used for regression are of the same spatial size (3 × 3) on the feature maps. To account for varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the k regressors do not share weights. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale, thanks to the design of anchors.

```python
class RPNHead(tf.keras.models.Model):
    def __init__(self, num_anchor_types, filters=256, kernel_size=3):
        self.input_layer = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same')
        # TODO: should there be a flatten here?
        self.box_head = tf.keras.layers.Dense(units=4 * num_anchor_types)
        self.class_head = tf.keras.layers.Dense(units=num_anchor_types)

    def __call__(self, inputs):
        inputs = self.input_layer(inputs)
        deltas = self.box_head(inputs)
        scores = self.class_head(inputs)
        return deltas, scores
```

## Anchor boxes

> [W]e introduce novel “anchor” boxes that serve as references at multiple scales and aspect ratios.

> An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left). By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position.

Each pixel in the input feature map corresponds to an anchor box. Here we will adapt FaceBook's MRCNN benchmark anchor generation code. First we define a single `anchor` with dimensions $[x_1, y_1, x_2, y_2] = [0, 0, s-1, s-1]$, where $s$ is the stride of the feature map with respect to the input.

> For anchors, we use 3 scales with box areas of 1282, 2562, and 5122 pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1. To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer ... the total stride ... on the last convolutional layer is 16 pixels.

> For a convolutional feature map of a size $W \times H$ ... there are $WHk$ anchors in total.

A grid of anchors is constructed by shifting the base anchor boxes across the feature map with a shift per pixel of the stride $s$. 

> The anchor boxes that cross image boundaries need
to be handled with care. During training, we ignore all cross-boundary anchors so they do not contribute to the loss. For a typical 1000 × 600 image, there will be roughly 20000 (≈ 60 × 40 × 9) anchors in total. With the cross-boundary anchors ignored, there are about 6000 anchors per image for training. 

## Feature Pyramid Networks

> Our goal is to leverage a ConvNet’s pyramidal feature
hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics through- out. The resulting Feature Pyramid Network is general- purpose and in this paper we focus on sliding window proposers (Region Proposal Network, RPN for short) [29] and region-based detectors (Fast R-CNN) [11]. We also generalize FPNs to instance segmentation proposals in Sec. 6. Our method takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures (e.g., [19, 36, 16]), and in this paper we present results using ResNets [16]. The construction of our pyramid involves a bottom-up pathway, a top-down pathway, and lat- eral connections, as introduced in the following.

# Bottom-up pathway

> The bottom-up pathway is the feed-forward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. There are often many layers producing output maps of the same size and we say these layers are in the same network stage. For our feature pyramid, we define one pyramid level for each stage. We choose the output of the last layer of each stage as our reference set of feature maps, which we will enrich to create our pyramid. This choice is natural since the deepest layer of each stage should have the strongest features.

```python
class BottomUpPyramid(tf.keras.Model):
    def __init__(self, config):
        super(BottomUpPyramid, self).__init__()
        backbone_model = config.fpn.backbone
        pyramid_names = config.fpn.pyramid_names
        backbone = getattr(tf.keras.applications, backbone_model)
        for layer in backbone.layers:
            layer.trainable = False

        # large -> small
        self.pyramid_layers = [backbone.get_layer(name) for name in pyramid_names]

    def __call__(self, inputs):
        pyramid = [pyramid_layer(inp)
                    for inp, pyramid_layer
                    in zip(self.pyramid_layers, inputs)]

        # large -> small
        return pyramid
```

> Specifically, for ResNets [16] we use the feature activations output by each stage’s last residual block. We denote the output of these last residual blocks as {C2, C3, C4, C5} for conv2, conv3, conv4, and conv5 outputs, and note that they have strides of {4, 8, 16, 32} pixels with respect to the input image. We do not include conv1 into the pyramid due to its large memory footprint. We denote the output of these last residual blocks as {C2, C3, C4, C5} for conv2, conv3, conv4, and conv5 outputs, and note that they have strides of {4, 8, 16, 32} pixels with respect to the input image. We do not include conv1 into the pyramid due to its large memory footprint.

# Top-down pathway

> The top- down pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.

> Fig. 3 shows the building block that constructs our top-down feature maps. With a coarser-resolution feature map, we upsample the spatial resolution by a factor of 2 (using nearest neighbor upsampling for simplicity). The upsampled map is then merged with the corresponding bottom-up channel dimensions) by element-wise addition. This process is iterated until the finest resolution map is generated.

```python
class TopDownPyramid(tf.keras.Model):
    def __init__(self, filters_in, bottom_up_filters, filters_out):
        super(TopDownPyramid, self).__init__()
        self.conv = tf.keras.layers.Conv2D(kernel_size=3, filters=filters_in, padding='same')
        self.fpn_blocks = [FPNBlock(filters_out, bu_filters) for bu_filters in bottom_up_filters]

    def __call__(self, inputs):
        x1 = self.conv(inputs[-1])
        pyramids = []
        # iterate from small -> large
        for x2, fpn_block in zip(inputs[::-1], self.fpn_blocks):
            x1 = fpn_block(x1, x2)
            pyramids.append(x1)

        # small -> large
        return pyramids
```

> Because all levels of the pyramid use shared classifiers/regressors as in a traditional featurized image pyramid, we fix the feature dimension (numbers of channels, denoted as d) in all the feature maps. We set d = 256 in this pa- per and thus all extra convolutional layers have 256-channel outputs. There are no non-linearities in these extra layers, which we have empirically found to have minor impacts.

> To start the iteration, we simply attach a 1×1 convolutional layer on C5 to produce the coarsest resolution map. Fi- generate the final feature map, which is to reduce the alias- nally, we append a 3×3 convolution on each merged map to ing effect of upsampling. This final set of feature maps is called {P2, P3, P4, P5}, corresponding to {C2, C3, C4, C5} that are respectively of the same spatial sizes

# Feature pyramid networks for RPN

> We adapt RPN by replacing the single-scale feature map with our FPN. We attach a head of the same design (3×3 conv and two sibling 1×1 convs) to each level on our feature pyramid. Because the head slides densely over all locations in all pyramid levels, it is not necessary to have multi-scale anchors on a specific level. Instead, we assign anchors of a single scale to each level. Formally, we define the anchors to have areas of $\{32^2, 64^2, 128^2, 256^2, 512^2\}$ pixels on $\{P_2, P_3, P_4, P_5, P_6\}$ respectively.1 As in [29] we also use anchors of multiple aspect ratios $\{1:2, 1:1, 2:1\}$ at each level. So in total there are 15 anchors over the pyramid.

> Here we introduce $P_6$ only for covering a larger anchor scale of $512^2$.
$P_6$ is simply a stride two subsampling of $P_5$. $P_6$ is not used by the Fast R-CNN detector.

```python
class FPN(tf.keras.Model):
    def __init__(self, config):
        super(FPN, self).__init__()
        self.bottom_up = BottomUpPyramid(**config.bottom_up)
        self.top_down = BottomUpPyramid(**config.top_down)

    def __call__(self, inputs):
        # large -> small
        # - - -
        #  - -
        #   -
        bottom_up = self.bottom_up(inputs)
        # small -> large
        #   -
        #  - -
        # - - -
        top_down = self.top_down(bottom_up)
        return top_down[::-1]
```



## RPN targets

# Intersection over Union
> IoU is an important criterion in object detection to compare predicted and target bounding boxes.

See this guide to implementing a vectorised version.

# Target assignment

>For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We assign a positive label to two kinds of anchors: 
>
>**(i) the anchor/anchors with the highest Intersection-over- Union (IoU) overlap with a ground-truth box, or** 
> 
>**(ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box.**
> [Emphasis added]


> Note that a single ground-truth box may assign positive labels to multiple anchors. Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample. We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. **Anchors that are neither positive nor negative do not contribute to the training objective.** [Emphasis added]

Note that by the max overlap condition the IoU can be less than 0.7 for a box to be considered positive.

These criteria tell us which of the anchors should have a positive and a negative label i.e. the classification targets but to get the regression targets we need to match the positive anchors to the bounding boxes. (Note that target boxes are undefined for negative anchors so we don't train the regression head for these). We will choose the highest overlap box as the target. 

```python
def make_targets(inds, values, num_rois):
    shape = tf.concat([[num_rois], tf.shape(values)[1:]], axis=0)
    inds = tf.cast(inds, tf.int32)[:, None]
    return tf.scatter_nd(inds, values, shape)


def rpn_targets(anchors, target_boxes, pos_iou_th=0.7, neg_iou_th=0.3, return_overlaps=False):

    anchors_shape = tf.shape(anchors)
    anchors = tf.reshape(anchors, (-1, 4))

    anchor_ax = -2
    box_ax = -1

    overlaps = box_iou(anchors, target_boxes)  # (A, B)
    # (B,)
    max_iou_bbox = tf.reduce_max(overlaps, axis=anchor_ax)
    # (A,)
    max_iou_anchors = tf.reduce_max(overlaps, axis=box_ax)
    n_boxes, n_anchors = tf.shape(overlaps)

    # Step 1 - assign to each bbox the anchor that has max IoU
    # (B,), values in the range [0, n_anchors)
    bbox_ids_all_bbox = tf.range(n_boxes)
    # (A,)
    anchor_ids_all_bbox = tf.argmax(overlaps, axis=anchor_ax)

    # Step 2 - for remaining anchors, assign to anchor_i to bbox_j if
    # j = argmax(overlaps[i, :]); AND EITHER
    # - overlaps[i, j] == max(overlaps[:, j]) - i.e. anchor_i has
    # an IoU with bbox_j equal to the max IoU bbox_j has with any anchor; OR
    # - overlaps[i, j] > pos_iou_th

    # (A',)
    anchors_left = tf.sets.difference(tf.range(n_anchors), anchor_ids_all_bbox)
    # (A',)
    max_iou_anchors_left = tf.gather(max_iou_anchors, anchors_left)
    # (M, )
    anchors_iou_above_th = tf.greater(max_iou_anchors_left, pos_iou_th)
    # (A', B) == (1, B) -> (A', B)
    # (A', B) -> (A',)
    anchors_iou_equals_max = tf.reduce_any(tf.equals(overlaps,  max_iou_bbox[None]), axis=box_ax)
    # (A',)
    anchors_pos_cond = tf.logical_or(anchors_iou_above_th, anchors_iou_equals_max)
    # (A'', 2) -> (A1,)
    anchors_ids_anchors_left = tf.where(anchors_pos_cond)[0]
    # (A1,)
    bbox_ids_anchors_left = tf.argmax(tf.gather(overlaps, anchors_ids_anchors_left), axis=-1)

    # (B + A1)
    pos_anchor_ids = tf.concat([anchor_ids_all_bbox, anchors_ids_anchors_left], axis=0)
    # (B + A1)
    pos_target_ids = tf.concat([bbox_ids_all_bbox, bbox_ids_anchors_left], axis=0)

    # (AA, 2) -> (AA,)
    maybe_neg_anchor_ids = tf.where(tf.less(max_iou_anchors, neg_iou_th), tf.float32)[0]
    # (A0,)
    neg_anchor_ids = tf.sets.difference(maybe_neg_anchor_ids, pos_anchor_ids)

    # (B + A1, 4)
    pos_targets = tf.gather(target_boxes, pos_target_ids)

    # (A, 4)
    boxes = make_targets(inds=pos_anchor_ids,
                         values=pos_targets,
                         num_rois=n_anchors)

    # (A,)
    labels = make_targets(inds=tf.concat([pos_anchor_ids, neg_anchor_ids], axis=0),
                          values=tf.concat([tf.ones_like(pos_anchor_ids),
                                            tf.negative(tf.ones_like(neg_anchor_ids))], axis=0),
                          num_rois=n_anchors)

    # (H, W, 4)
    boxes = tf.reshape(boxes, anchors_shape)
    # (H, W)
    labels = tf.reshape(labels, anchors_shape[:-1])

    return (boxes, labels) + (overlaps if return_overlaps else ())
```

# Regression targets / "deltas"

>For bounding box regression, we adopt the param- eterizations of the 4 coordinates following [5]:
>
>$$t_x = (x − x_a)/w_a,\text{ }\text{ }t_y = (y − y_a)/h_a \\
>t_w = \log(w/w_a),\text{ }\text{ }t_h = \log(h/h_a) \\
>t_x = (x^∗ − x_a)/w_a,\text{ }\text{ }t^∗_y = (y^∗ − y_a)/h_a \\
>t^∗_w = \log(w^∗/w_a),\text{ }\text{ }t^∗_y = (y^∗ − y_a)/h_a \\
>t^*_h = \log(h^∗/h_a),\text{ }\text{ }t^∗_h = (h^∗/h_a)$$
>
>where $x$, $y$, $w$, and $h$ denote the box’s center coordi- nates and its width and height. Variables $x$, $x_a$, and $x^∗$ are for the predicted box, anchor box, and ground- truth box respectively (likewise for $y$, $w$, $h$). This can be thought of as bounding-box regression from an anchor box to a nearby ground-truth box

The outputs of the model are interpreted as deltas and targets are converted into deltas for calculating the loss. Since the model outputs deltas, to use them as boxes, we also need a function to convert the deltas into back into boxes.

Read more about deltas in the guide to bounding boxes.

## Training RPNs

>The RPN can be trained end-to-end by back- propagation and stochastic gradient descent (SGD) [35]. We follow the “image-centric” sampling strategy from [2] to train this network. Each mini-batch arises from a single image that contains many positive and negative example anchors. It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to $1:1$. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones.

```python
def sample_rois(instance, indices, num_samples, pos_fraction, pos_id=1, neg_id=-1):
    samples = []
    pos_inds = tf.boolean_mask(indices, tf.equal(indices, pos_id))
    neg_inds = tf.boolean_mask(indices, tf.equal(indices, neg_id))
    num_pos = tf.cast(tf.round(num_samples * pos_fraction), tf.int32)
    num_neg = num_samples - tf.minimum(num_pos, tf.shape(pos_inds)[0])
    for inds, num in zip([pos_inds, neg_inds], [num_pos, num_neg]):
        sample_inds = tf.random.shuffle(inds)[:num]
        samples.append(instance.select(sample_inds))
    samples = instance_concat(samples)
    return samples


def sample_rpn_rois(instance, num_samples=256, pos_fraction=0.5):
    inside = tf.equal(instance, 1)
    pos_inds, neg_inds = [
        tf.where(tf.logical_and(tf.equal(instance.target_labels, label), inside))
        for label in [1, -1]
    ]
    return sample_rois(instance, pos_inds, neg_inds, num_samples, pos_fraction)

```

>Our loss function for an image is defined as
>
>$$L(\{p_i\}, \{t_i\}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^*) + \lambda\frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^*) $$
>
>Here, i is the index of an anchor in a mini-batch and pi is the predicted probability of anchor i being an object. The ground-truth label $p_i^∗$ is 1 if the anchor
>is positive, and is 0 if the anchor is negative. ti is a vector representing the 4 parameterized coordinates of the predicted bounding box, and $t_i^∗$  is that of the
>ground-truth box associated with a positive anchor. The classification loss Lcls is log loss over two classes (object vs. not object). 

```python
def rpn_class_loss(samples):
    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(
        logits=samples.scores,
        labels=tf.cast(tf.equal(samples.target_labels, 1), tf.int32)
    )
    # Mean equivalent to normalizing by mini-batch size
    return tf.reduce_mean(losses)
```

> For the regression loss, we use $L\_{reg}(t\_i, t\_i^∗) = R(t\_i − t\_i^*)$ where $R$ is the robust loss function (smooth $L\_1$) defined in [2]. The term $p\_i^∗ L\_{reg}$ means the regression loss is activated only for positive anchors $(p\_i^\* = 1)$ and is disabled otherwise $(p\_i^\* = 0)$. The outputs of the cls and reg layers consist of $\{p\_i\}$ and $\{t\_i\}$ respectively.

```python
def rpn_reg_loss(samples, denom=None, reg_lambda=None):
    samples = samples.masked_select(tf.equal(samples.target_labels, 1))
    losses = tf.losses.huber_loss(predictions=samples.deltas,
                                  labels=samples.target_boxes,
                                  reduction=None)
    if reg_lambda is not None:
        losses = losses * reg_lambda

    if denom is not None:
        return tf.reduce_sum(losses) / denom

    return tf.reduce_mean(losses)
```

>The two terms are normalized by $N_{cls}$ and $N_{reg}$ and weighted by a balancing parameter $\lambda$. In our current implementation (as in the released code), the cls term in Eqn.(1) is normalized by the mini-batch size (i.e., $N_{cls}$ = 256) and the reg term is normalized by the number of anchor locations (i.e., $N_{reg}$ = 2400). By default we set $\lambda = 10$, and thus both cls and reg terms are roughly equally weighted. We show by experiments that the results are insensitive to the values of $\lambda$ in a wide range (Table 9). We also note that the normalization as above is not required and could be simplified

```python
def rpn_loss(instance, num_samples, reg_lambda=None, num_reg=False):
    samples = sample_rpn_rois(instance, num_samples)
    if num_reg:
        denom = tf.shape(instance.deltas)[0]
    else:
        denom = None
    return {'rpn_class_loss': rpn_class_loss(samples),
            'rpn_reg_loss': rpn_reg_loss(samples, denom, reg_lambda)}
```

# Non-maximum suppression

>To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image.

```python
def instance_nms(instance, max_output_size, **nms_kwargs):
    keep = tf.image.non_max_suppression(
        instance.boxes,
        instance.scores,
        max_output_size=max_output_size,
        **nms_kwargs
    )
    instance = instance.select(keep)
    instance = instance.pad(max_output_size)
    return instance
```

## Putting it all together
Given a config object, a model and feature maps, let us build an RPN

```python
def build_rpn(config, model, inputs, gt_instance=None, mode='train'):
    anchors, inside = pyramid_anchors(config.rpn.strides,
                                      config.rpn.sizes,
                                      config.rpn.aspect_ratios,
                                      config.rpn.img_width,
                                      config.rpn.img_height)
    anchors = tf.reshape(anchors, [-1, 4])
    inside = tf.reshape(inside, [-1])
    deltas, scores = model(inputs)

    proposals = instance_nms(
        Instance(dict(
            scores=scores,
            boxes=delta2box(deltas, anchors)
        )).boxes,
        **config.rpn.nms_kwargs
    )

    if mode == 'test':
        return proposals

    target_boxes, target_labels = rpn_targets(anchors, gt_instance.target_boxes,
                                              pos_iou_th=config.rpn.pos_iou_th,
                                              neg_iou_th=config.rpn.neg_iou_th)
    instance = Instance(dict(scores=scores,
                            anchors=anchors,
                            inside=inside,
                            deltas=deltas,
                            target_boxes=target_boxes,
                            target_labels=target_labels))

    return proposals, instance
```


## Fast R-CNN
Coming soon!

<div>
<!-- ### Overview
>A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map. Then, for each ob- ject proposal a region of interest (RoI) pooling layer ex- tracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output lay- ers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.

<!-- ## RoI features
Coming soon!

## RoIAlign
Coming soon! -->

<!-- # Detector --> 

<!-- >A Fast R-CNN network has two sibling output layers. The first outputs a discrete probability distribution (per RoI), p = (p0, . . . , pK), over K + 1 categories. As usual, p is computed by a softmax over the K+1 outputs of a fully connected layer. The second sibling layer outputs bounding-box regression offsets, tk =
? tk x , tk y , tk w, tk h ? , for
each of the K object classes, indexed by k. We

We will use the configuration from Mask R-CNN

>Mask R-CNN adopts the same two-stage procedure, with an identical first stage (which is RPN).

>Figure 4. Head Architecture: We extend two existing Faster R- CNN heads [19, 27]. Left/Right panels show the heads for the ResNet C4 and FPN backbones, from [19] and [27], respectively, to which a mask branch is added. Numbers denote spatial resolu- tion and channels. Arrows denote either conv, deconv, or fc layers as can be inferred from context (conv preserves spatial dimension while deconv increases it). All convs are 3×3, except the output conv which is 1×1, deconvs are 2×2 with stride 2, and we use ReLU [31] in hidden layers. Left: ‘res5’ denotes ResNet’s fifth stage, which for simplicity we altered so that the first conv oper- ates on a 7×7 RoI with stride 1 (instead of 14×14 / stride 2 as in [19]). Right: ‘×4’ denotes a stack of four consecutive convs.
3.1.

TODO: add figure

Here we will implement the one for FPN -->

<!-- >**Multinomial vs. Independent Masks:** Mask R-CNN de- couples mask and class prediction: as the existing box branch predicts the class label, we generate a mask for each class without competition among classes (by a per-pixel sig- moid and a binary loss). In Table 2b, we compare this to using a per-pixel softmax and a multinomial loss (as com- monly used in FCN [30]). This alternative couples the tasks of mask and class prediction, and results in a severe loss in mask AP (5.5 points). This suggests that once the in- stance has been classified as a whole (by the box branch), it is sufficient to predict a binary mask without concern for the categories, which makes the model easier to train.
>
>**Class-Specific vs. Class-Agnostic Masks:** Our default in- stantiation predicts class-specific masks, i.e., one m×m mask per class. Interestingly, Mask R-CNN with class- agnostic masks (i.e., predicting a single m×m output re- gardless of class) is nearly as effective: it has 29.7 mask AP vs. 30.3 for the class-specific counterpart on ResNet-50-C4. This further highlights the division of labor in our approach which largely decouples classification and segmentation.

Due to the inferior performance of multinomial masks we will only implement independent masks with the option for class-agnostic masks since they have a comparable performance to class-specific ones.

## Detection targets
>As in [9], we take $25\%$ of the RoIs from object proposals that have intersection over union (IoU) overlap with a ground- truth bounding box of at least $0.5$. These RoIs comprise the examples labeled with a foreground object class, i.e. $u ≥ 1$. The remaining RoIs are sampled from object proposals that have a maximum IoU with ground truth in the interval $[0.1, 0.5)$, following [11]. These are the background examples and are labeled with $u = 0$. The lower threshold of $0.1$ appears to act as a heuristic for hard example mining [8]. -->

<!-- ## Training 
Coming soon!

### Image-centric training
>We propose a more efficient training method that takes advantage of feature sharing during training. In Fast R- CNN training, stochastic gradient descent (SGD) mini- batches are sampled hierarchically, first by sampling N im- ages and then by sampling R/N RoIs from each image. Critically, RoIs from the same image share computation and memory in the forward and backward passes. Making N small decreases mini-batch computation. For example, when using N = 2 and R = 128, the proposed training scheme is roughly 64× faster than sampling one RoI from 128 different images (i.e., the R-CNN and SPPnet strategy). One concern over this strategy is it may cause slow train-
ing convergence because RoIs from the same image are cor- related. This concern does not appear to be a practical issue  -->

<!-- ### Mini-batch sampling
>Each mini-batch has 2 images per GPU and each image has N sampled RoIs, with a ratio of 1:3 of positive to negatives [12]. N is 64 for the C4 backbone (as in [12, 36]) and 512 for FPN (as in [27]).
>
>We use a multi-task loss L on each labeled RoI to jointly train for classification and bounding-box regression:

>$$ L(p, u, t_u, v) = L_\text{cls}(p, u) + \lambda[u ≥ 1]L_\text{loc}(t_u, v) $$

For Mask R-CNN

>Formally, during training, we define a multi-task loss on each sampled RoI as $L = L_\text{cls} + L_\text{box} + L_\text{mask}$. The classification loss Lcls and bounding-box loss Lbox are identi- cal as those defined in [12]. The mask branch has a Km2- dimensional output for each RoI, which encodes K binary masks of resolution m×m, one for each of the K classes. To this we apply a per-pixel sigmoid, and define Lmask as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, Lmask is only defined on the k-th mask (other mask outputs do not contribute to the loss). -->
</div>